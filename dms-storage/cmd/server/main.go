package main

import (
	"context"
	"encoding/json"
	"fmt"
	"log"
	"log/slog"
	"net/http"
	"time"

	"github.com/gofiber/fiber/v2"
	"github.com/segmentio/kafka-go"

	"go.opentelemetry.io/contrib/bridges/otelslog"
	"go.opentelemetry.io/otel"
	"go.opentelemetry.io/otel/attribute"
	"go.opentelemetry.io/otel/codes"
	"go.opentelemetry.io/otel/exporters/otlp/otlplog/otlploggrpc"
	"go.opentelemetry.io/otel/exporters/otlp/otlptrace/otlptracegrpc"
	"go.opentelemetry.io/otel/log/global"
	"go.opentelemetry.io/otel/propagation"
	sdklog "go.opentelemetry.io/otel/sdk/log"
	"go.opentelemetry.io/otel/sdk/resource"
	sdktrace "go.opentelemetry.io/otel/sdk/trace"
	semconv "go.opentelemetry.io/otel/semconv/v1.4.0"
	"go.opentelemetry.io/otel/trace"
)

var tracer = otel.Tracer("dms-storage-worker")

// ============================================
// [NEW] Kafka Writer (Producer) ì „ì—­ ë³€ìˆ˜
// Redis Client ëŒ€ì‹  Kafka Writer ì‚¬ìš©
// ============================================
var kafkaWriter *kafka.Writer

type TaskRequest struct {
	DocTitle string `json:"doc_title"`
	Action   string `json:"action"`
}

func main() {
	ctx := context.Background()

	// ============================================
	// [ë³€ê²½] Redis â†’ Kafka Writer ì´ˆê¸°í™”
	// ============================================
	kafkaWriter = &kafka.Writer{
		Addr:     kafka.TCP("localhost:9092"), // Kafka ë¸Œë¡œì»¤ ì£¼ì†Œ
		Topic:    "task.complete",             // Topic ì´ë¦„ (Redisì˜ ì±„ë„ê³¼ ë™ì¼í•œ ê°œë…)
		Balancer: &kafka.LeastBytes{},         // íŒŒí‹°ì…˜ ë¶„ë°° ì „ëµ: ê°€ì¥ ì ê²Œ ì“´ íŒŒí‹°ì…˜ìœ¼ë¡œ

		// ì„±ëŠ¥ íŠœë‹ ì˜µì…˜
		BatchSize:    100,                   // 100ê°œì”© ëª¨ì•„ì„œ í•œ ë²ˆì— ì „ì†¡ (íš¨ìœ¨ì„± UP)
		BatchTimeout: 10 * time.Millisecond, // 100ê°œ ì•ˆ ëª¨ì—¬ë„ 10ms ì§€ë‚˜ë©´ ì „ì†¡
		Compression:  kafka.Snappy,          // ì••ì¶• (ë„¤íŠ¸ì›Œí¬ íŠ¸ë˜í”½ ê°ì†Œ)

		// ì „ë‹¬ ë³´ì¥ ë ˆë²¨ ì„¤ì •
		RequiredAcks: kafka.RequireOne, // 1ê°œ ë¸Œë¡œì»¤ë§Œ í™•ì¸í•˜ë©´ OK (ë¹ ë¦„)
		// RequiredAcks: kafka.RequireAll,     // ëª¨ë“  ë³µì œë³¸ í™•ì¸ (ëŠë¦¬ì§€ë§Œ ì•ˆì „)

		// ì—ëŸ¬ ì²˜ë¦¬
		MaxAttempts: 3, // ì „ì†¡ ì‹¤íŒ¨ ì‹œ 3ë²ˆê¹Œì§€ ì¬ì‹œë„
	}
	defer kafkaWriter.Close() // í”„ë¡œê·¸ë¨ ì¢…ë£Œ ì‹œ ì •ë¦¬

	// Trace ì´ˆê¸°í™”
	shutdownTracer := initTracer()
	defer shutdownTracer(ctx)

	// Log ì´ˆê¸°í™”
	shutdownLogger := initLogger()
	defer shutdownLogger(ctx)

	logger := otelslog.NewLogger("dms-storage-worker-logger")
	slog.SetDefault(logger)

	app := fiber.New(fiber.Config{
		AppName: "DMS Storage Worker",
	})

	// ============================================
	// ë¯¸ë“¤ì›¨ì–´: Ginì´ ë³´ë‚¸ TraceID ë°›ê¸°
	// ============================================
	app.Use(func(c *fiber.Ctx) error {
		carrier := &FiberHeaderCarrier{c: c}
		extractedCtx := otel.GetTextMapPropagator().Extract(context.Background(), carrier)

		spanCtx, span := tracer.Start(extractedCtx, c.Path(),
			trace.WithAttributes(
				attribute.String("http.method", c.Method()),
				attribute.String("http.url", c.OriginalURL()),
			),
		)

		c.SetUserContext(spanCtx)
		err := c.Next()

		if err != nil {
			span.RecordError(err)
			span.SetStatus(codes.Error, err.Error())
		} else {
			span.SetStatus(codes.Ok, "success")
		}
		span.SetAttributes(attribute.Int("http.status_code", c.Response().StatusCode()))
		span.End()

		return err
	})

	app.Post("/internal/process", func(c *fiber.Ctx) error {
		ctx := c.UserContext()
		span := trace.SpanFromContext(ctx)
		traceID := span.SpanContext().TraceID().String()

		req := new(TaskRequest)

		if err := c.BodyParser(req); err != nil {
			span.RecordError(err)
			span.SetStatus(codes.Error, "íŒŒì‹± ì‹¤íŒ¨")
			slog.ErrorContext(ctx, "Body Parsing Failed", "error", err)
			return c.Status(400).JSON(fiber.Map{"error": "íŒŒì‹± ì‹¤íŒ¨"})
		}

		slog.InfoContext(ctx, "Fiber ì‘ì—… ì‹œì‘",
			"action", req.Action,
			"doc_title", req.DocTitle,
			"manual_trace_id", traceID,
		)

		fmt.Printf("âš¡ [Fiber] ì‘ì—… ì²˜ë¦¬ ì¤‘... TraceID: %s\n", traceID)

		// ì„¸ë¶€ ì‘ì—… ì¶”ì  Span
		_, processSpan := tracer.Start(ctx, "process_document",
			trace.WithAttributes(
				attribute.String("doc_title", req.DocTitle),
				attribute.String("action", req.Action),
			),
		)

		time.Sleep(100 * time.Millisecond) // ì‘ì—… ì‹œëŠ‰
		processSpan.End()

		fmt.Printf("âœ… [Fiber] ì™„ë£Œ!\n")

		// ============================================
		// [ë³€ê²½] Redis Publish â†’ Kafka ë©”ì‹œì§€ ë°œí–‰
		// ============================================
		go publishToKafka(ctx, traceID)

		return c.Status(http.StatusOK).JSON(fiber.Map{
			"status": "PROCESSED",
		})
	})

	fmt.Println("âš¡ Fiber Storage Worker running on :8081")
	app.Listen(":8081")
}

// ============================================
// [NEW] Kafkaë¡œ ë©”ì‹œì§€ ë°œí–‰í•˜ëŠ” í•¨ìˆ˜
// ê¸°ì¡´: rdb.Publish()
// ë³€ê²½: kafkaWriter.WriteMessages()
// ============================================
func publishToKafka(ctx context.Context, traceID string) {
	// ë©”ì‹œì§€ êµ¬ì¡°ì²´ ìƒì„± (Redisì™€ ë™ì¼í•œ ì–‘ì‹)
	event := map[string]string{
		"trace_id": traceID,
		"status":   "DONE",
		"message":  "Kafkaë¡œ ì•Œë¦¼ ë³´ëƒ„",
	}

	jsonBody, err := json.Marshal(event)
	if err != nil {
		slog.Error("JSON ë§ˆìƒ¬ë§ ì‹¤íŒ¨", "error", err)
		return
	}

	// Kafka ë©”ì‹œì§€ ìƒì„±
	msg := kafka.Message{
		// Key: ê°™ì€ KeyëŠ” ê°™ì€ íŒŒí‹°ì…˜ìœ¼ë¡œ ê°€ì„œ ìˆœì„œ ë³´ì¥ë¨
		// traceIDë¥¼ Keyë¡œ ì“°ë©´ ê°™ì€ ì‘ì—…ì€ ìˆœì„œëŒ€ë¡œ ì²˜ë¦¬ë¨
		Key: []byte(traceID),

		// Value: ì‹¤ì œ ë©”ì‹œì§€ ë‚´ìš© (JSON)
		Value: jsonBody,

		// Time: ë©”ì‹œì§€ ìƒì„± ì‹œê°„ (Kafkaê°€ ìë™ ê¸°ë¡ë„ í•˜ì§€ë§Œ ëª…ì‹œ ê°€ëŠ¥)
		Time: time.Now(),

		// Headers: ì¶”ê°€ ë©”íƒ€ë°ì´í„° (ì„ íƒì‚¬í•­)
		// ì˜ˆ: OpenTelemetry TraceIDë¥¼ í—¤ë”ì—ë„ ë„£ì„ ìˆ˜ ìˆìŒ
		Headers: []kafka.Header{
			{Key: "trace-id", Value: []byte(traceID)},
		},
	}

	// Kafkaë¡œ ë©”ì‹œì§€ ì „ì†¡
	// WriteMessagesëŠ” ë™ê¸° ë°©ì‹ (ì „ì†¡ ì™„ë£Œê¹Œì§€ ëŒ€ê¸°)
	err = kafkaWriter.WriteMessages(ctx, msg)

	if err != nil {
		// ì „ì†¡ ì‹¤íŒ¨ ì‹œ (ë„¤íŠ¸ì›Œí¬ ë¬¸ì œ, Kafka ë‹¤ìš´ ë“±)
		slog.ErrorContext(ctx, "Kafka ì „ì†¡ ì‹¤íŒ¨",
			"error", err,
			"trace_id", traceID,
		)
		return
	}

	// ì„±ê³µ ë¡œê·¸
	fmt.Printf("ğŸ“¤ [Kafka] ë©”ì‹œì§€ ë°œí–‰ ì™„ë£Œ (TraceID: %s)\n", traceID)
	slog.InfoContext(ctx, "Kafka ë°œí–‰ ì„±ê³µ",
		"trace_id", traceID,
		"topic", "task.complete",
	)
}

// FiberHeaderCarrier êµ¬ì¡°ì²´ (ë³€ê²½ ì—†ìŒ)
type FiberHeaderCarrier struct {
	c *fiber.Ctx
}

func (f *FiberHeaderCarrier) Get(key string) string { return f.c.Get(key) }
func (f *FiberHeaderCarrier) Set(key, value string) { f.c.Set(key, value) }
func (f *FiberHeaderCarrier) Keys() []string {
	keys := make([]string, 0)
	f.c.Request().Header.VisitAll(func(key, value []byte) {
		keys = append(keys, string(key))
	})
	return keys
}

// Tracer ì´ˆê¸°í™” (ë³€ê²½ ì—†ìŒ)
func initTracer() func(context.Context) error {
	ctx := context.Background()
	exporter, err := otlptracegrpc.New(ctx,
		otlptracegrpc.WithInsecure(),
		otlptracegrpc.WithEndpoint("localhost:4317"),
	)
	if err != nil {
		log.Fatalf("Trace Init Error: %v", err)
	}

	res, _ := resource.New(ctx,
		resource.WithAttributes(
			semconv.ServiceNameKey.String("dms-storage-worker"),
		),
	)

	tp := sdktrace.NewTracerProvider(
		sdktrace.WithBatcher(exporter),
		sdktrace.WithResource(res),
	)
	otel.SetTracerProvider(tp)
	otel.SetTextMapPropagator(propagation.NewCompositeTextMapPropagator(
		propagation.TraceContext{},
		propagation.Baggage{},
	))

	return tp.Shutdown
}

// Logger ì´ˆê¸°í™” (ë³€ê²½ ì—†ìŒ)
func initLogger() func(context.Context) error {
	ctx := context.Background()

	exporter, err := otlploggrpc.New(ctx,
		otlploggrpc.WithInsecure(),
		otlploggrpc.WithEndpoint("localhost:4317"),
	)
	if err != nil {
		log.Fatalf("Log Init Error: %v", err)
	}

	res, _ := resource.New(ctx,
		resource.WithAttributes(
			semconv.ServiceNameKey.String("dms-storage-worker"),
		),
	)

	lp := sdklog.NewLoggerProvider(
		sdklog.WithProcessor(sdklog.NewBatchProcessor(exporter)),
		sdklog.WithResource(res),
	)

	global.SetLoggerProvider(lp)

	return lp.Shutdown
}
